{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1344,"status":"ok","timestamp":1707051684483,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"},"user_tz":-420},"id":"k5zFKN-Qhu1e","outputId":"4a2391af-a804-4189-bb7a-24f5f3effdd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["EURCHF\n"]}],"source":["# Tổng quan bước này ta lấy dữ liệu từ mt5\n","# Sau đó lưu vào biến tên là data_forex\n","# Và nhớ khai báo các cặp tiền tệ cần thiết vào biến symbols\n","\n","import requests\n","import pandas as pd\n","from datetime import datetime\n","\n","\"\"\"Cảnh báo không được thay đổi địa chỉ ip của hàm này\"\"\"\n","def get_history(symbol: str, timeframe: str = \"D1\", start: str = \"2014-01-01\", end: str = datetime.now().date().isoformat()):\n","    params = {\n","        \"timeframe\": timeframe,\n","        \"start\": start,\n","        \"end\": end,\n","    }\n","    response = requests.get(f\"http://221.132.33.180:8005/history/{symbol}\", params=params)\n","    response.raise_for_status()\n","    data = response.json()\n","    data = pd.DataFrame(data)\n","    data[\"Date\"] = pd.to_datetime(data['Date'], unit='s')\n","    data.set_index(\"Date\", inplace=True)\n","    return data\n","\n","# Phía trên là hàm lấy dữ liệu từ mt5  và đầu vào là cạp tiền tệ của forex, crypto, stock\n","# hàm trả về data của symbols đó\n","\n","# symbols = {'all': [\"EURCHF\", \"EURNZD\", \"NZDUSD\", \"AUDNZD\", \"USDJPY\", \"NZDJPY\", \"GBPJPY\", \"USDCHF\"]}\n","symbols = {'all': [\"EURCHF\"]}\n","\"\"\"Đoạn code này có thể chỉnh sửa lấy loại tiền tệ theo yêu cầu\"\"\"\n","def get_data(symbols):\n","  data_forex = {}\n","  for sym in symbols[\"all\"]:\n","    data_forex[sym] = get_history(sym)\n","    print(sym)\n","  return data_forex\n","\n","data_forex = get_data(symbols)"]},{"cell_type":"markdown","metadata":{"id":"ycGmMYMbUUe1"},"source":["# Bước lưu lại dữ liệu và load dữ liệu lên nếu cần"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUkgb76p2hXX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wcAM8zBvjCuJ"},"source":["# Code này là chia sẽ cho mọi người sử dụng chung code để có thể làm việc với forex và sau đó hiểu được code"]},{"cell_type":"markdown","metadata":{"id":"4e6osb-jjK_T"},"source":["# Điều quan trọng khi làm trong code là tinh gọn code, code chạy auto, chạy theo tùng hàm và cuối cùng là đầu vào đơn giản của từng hàm.\n"]},{"cell_type":"markdown","metadata":{"id":"_-eUhgjilWce"},"source":["\n","\n","- Các bước sắp xếp code trong dữ liệu:\n","1. Khai báo những thư viên công cụ được sử dụng. Lấy dữ liệu từ mt5.\n","2. Các bước xử lý dữ liệu gồm:  Làm sạch, lọc nhiễu, smooth, loại bỏ outlier.\n","3. Khi có dữ liệu ta bắt đầu xem các model được sử dụng: Bước Tuning hyperparameter, bước thứ hai là train model.\n","4. Bước thứ 4 là đưa ra dự đoán, xem lợi nhuận của từng đồng tiền.\n","5. Lưu lại tất cả những gì đã làm từ model, từ paramete,.. và các thứ khác có thể add vào model\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fslnd1lwwP5_","outputId":"edc5f398-509a-4a62-881e-d41e3736cd4b","executionInfo":{"status":"ok","timestamp":1707051504509,"user_tz":-420,"elapsed":16485,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting finta\n","  Downloading finta-1.3-py3-none-any.whl (29 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from finta) (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from finta) (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->finta) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->finta) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->finta) (1.16.0)\n","Installing collected packages: finta\n","Successfully installed finta-1.3\n","Collecting simdkalman\n","  Downloading simdkalman-1.0.4-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from simdkalman) (1.23.5)\n","Installing collected packages: simdkalman\n","Successfully installed simdkalman-1.0.4\n","Collecting skorch\n","  Downloading skorch-0.15.0-py3-none-any.whl (239 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.23.5)\n","Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n","Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.2.0)\n","Installing collected packages: skorch\n","Successfully installed skorch-0.15.0\n"]}],"source":["!pip install finta\n","!pip install simdkalman\n","!pip install skorch"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"blCLOfRTwUa-","executionInfo":{"status":"ok","timestamp":1707051511138,"user_tz":-420,"elapsed":6637,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import yaml\n","import matplotlib.pyplot as plt\n","import pickle\n","\n","from datetime import datetime\n","from collections import Counter\n","from finta import TA\n","from scipy.signal import savgol_filter\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import KNNImputer, SimpleImputer\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","from tqdm import tqdm\n","import os\n","from pathlib import Path\n","import glob\n","# thư viện làm trơn nhãn\n","import simdkalman\n","from sklearn.ensemble import IsolationForest\n","import plotly.express as px\n","# thư viện tuning\n","from sklearn.neural_network import MLPClassifier\n","\n","# dùng torch làm cái mlp là 1 model phân lớp\n","import torch\n","from torch import nn\n","from skorch import NeuralNetClassifier\n","from skorch.callbacks import EarlyStopping\n","from skorch.callbacks import Checkpoint\n","# dùng xgboost làm 1 model phân lớp\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import KFold\n","from tqdm import notebook\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import log_loss\n","from sklearn.metrics import roc_auc_score\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier"]},{"cell_type":"markdown","metadata":{"id":"L8DH6u3-WL9x"},"source":["# Công đoạn tiếp theo là ta sẽ xử lý dữ liệu"]},{"cell_type":"markdown","metadata":{"id":"-Q2i_Z09wtgJ"},"source":["# Công đoạn này thêm các technical indicators vào"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"zf5m8YCtlVl7","executionInfo":{"status":"ok","timestamp":1707051826740,"user_tz":-420,"elapsed":4,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}}},"outputs":[],"source":["# Hàm khởi tạo các  technical indicator\n","def create_indicators(ohlcv: pd.DataFrame) -> pd.DataFrame:\n","    data = ohlcv.copy()\n","    indi, signal = bias(ohlcv)\n","    data = pd.concat([data, indi], axis=1)\n","    data['BIAS_signal'] = signal\n","    data['VR'], data[\"VR_signal\"] = vr(ohlcv)\n","    data['TRIX'], data[\"TRIX_signal\"] = trix(ohlcv)\n","    data['ER'] = TA.ER(ohlcv)\n","    data['EVWMA'] = TA.EVWMA(ohlcv)\n","    data['VWAP'] = TA.VWAP(ohlcv)\n","    data['MOM'] = TA.MOM(ohlcv)\n","    data['ROC'] = TA.ROC(ohlcv)\n","    data['RSI'] = TA.RSI(ohlcv)\n","    data['IFT_RSI'] = TA.IFT_RSI(ohlcv)\n","    data['ATR'] = TA.ATR(ohlcv)\n","    data['BBWIDTH'] = TA.BBWIDTH(ohlcv)\n","    data['ADX'] = TA.ADX(ohlcv)\n","    data['STOCH'] = TA.STOCH(ohlcv)\n","    data['STOCHD'] = TA.STOCHD(ohlcv)\n","    data['AO'] = TA.AO(ohlcv)\n","    data['MI'] = TA.MI(ohlcv)\n","    data['MFI'] = TA.MFI(ohlcv)\n","    data['PZO'] = TA.PZO(ohlcv)\n","    data['EFI'] = TA.EFI(ohlcv)\n","    data['EMV'] = TA.EMV(ohlcv)\n","    data['CCI'] = TA.CCI(ohlcv)\n","    data['FISH'] = TA.FISH(ohlcv)\n","    data['FVE'] = TA.FVE(ohlcv)\n","\n","    macd = TA.MACD(ohlcv)\n","    data['MACDCal'] = macd['MACD'] - macd['SIGNAL']\n","\n","    macdev = TA.EV_MACD(ohlcv)\n","    data['EVMACD'] = macdev[\"MACD\"]\n","\n","    data['TR'] = TA.TR(ohlcv)\n","\n","    DMI = TA.DMI(ohlcv)\n","    data['DMI+'] = DMI[\"DI+\"]\n","    data['DMI-'] = DMI[\"DI-\"]\n","\n","    VORTEX = TA.VORTEX(ohlcv)\n","    data['VIp'] = VORTEX[\"VIp\"]\n","    data['ADL'] = TA.ADL(data)\n","\n","    TSI = TA.TSI(ohlcv)\n","    data['TSI'] = TSI[\"TSI\"]\n","    data['TSIsignal'] = TSI[\"signal\"]\n","\n","    KST = TA.KST(ohlcv)\n","    data['KST'] = KST[\"KST\"]\n","\n","    data['CHAIKIN'] = TA.CHAIKIN(ohlcv)\n","    data['OBV'] = TA.OBV(ohlcv)\n","    data['WOBV'] = TA.WOBV(ohlcv)\n","\n","    EBBP = TA.EBBP(ohlcv)\n","    data['EBBPBull'] = EBBP[\"Bull.\"]\n","    data['EBBPBear'] = EBBP[\"Bear.\"]\n","\n","    BASPN = TA.BASPN(ohlcv)\n","    data['BASPNBuy'] = BASPN[\"Buy.\"]\n","    data['BASPNSell'] = BASPN[\"Sell.\"]\n","    data['COPP'] = TA.COPP(ohlcv)\n","\n","    BASP = TA.BASP(ohlcv)\n","    data['BASPBuy'] = BASP[\"Buy.\"]\n","    data['BASPSell'] = BASP[\"Sell.\"]\n","\n","    WTO = TA.WTO(ohlcv)\n","    data['WTOWT1'] = WTO[\"WT1.\"]\n","\n","    data['STC'] = TA.STC(ohlcv)\n","    data['VPT'] = TA.VPT(ohlcv)\n","\n","    # Có 50 kỹ thuật technical indicators ở đây\n","    return data\n","\n","LOOK_BACK = 10\n","\"\"\"Hàm này dùng để phân chia data ra theo từng ngày nối đuôi nhau vựa trên lock_back\"\"\"\n","# Xem xem trung bình 10 ngày tăng hay giảm, là dùng để xem xu hướng của tiền\n","\n","def create_dataset(data, label, look_back=1):  #\n","    X_ = []\n","    y_ = []\n","    # Tạo vòng lạp để lấy các lookback\n","    for i in range(len(data)-look_back-1):\n","        X_.append(data[i:(i+look_back)])\n","        y_.append(label[i + look_back])\n","    return np.array(X_), np.array(y_)\n","\n","\"\"\" Hàm này để lấy các ngày liên tiếp theo n_days để làm các cột dữ liệu\"\"\"\n","\n","def add_past_days_as_feature(data: pd.DataFrame, n_days: int = 5):\n","    data = pd.concat([data.shift(i).add_suffix(f\"_{i}\") for i in range(n_days)], axis=1)\n","    return data\n","\n","\" Hàm Bias dùng để so sánh giữ hai hai tính hiệu dài hạn và ngắn hạn vựa trên short_val và long_val \"\n","def bias(prices):\n","    short_avg = prices['Close'].rolling(3, min_periods=1).mean()\n","    long_avg = prices['Close'].rolling(5, min_periods=1).mean()\n","\n","    short_val = pd.Series(((prices['Close'] - short_avg) / short_avg) * 100, name=\"BIAS_short\", index=prices.index)\n","    long_val = pd.Series(((prices['Close'] - long_avg) / long_avg) * 100, name=\"BIAS_long\", index=prices.index)\n","    indi = pd.concat([short_val, long_val], axis=1)\n","\n","    # So sánh xem dài hạn hay ngắn hạn cái nào sẽ lời hơn\n","    signal = pd.Series((long_val > short_val).astype(int), name=\"BIAS_signal\", index=prices.index)\n","    return indi, signal\n","\n","def vr(prices):\n","    maximum = (prices['High'] + prices['Close'].shift(1).bfill()).mean() # Lấy giá cao nhất ngày hiện tạo cộng cho ngày đóng cửa của tương lai,\n","    # Nếu tương lai là Nan thì cộng cho giá đóng cưa hiện tại\n","    minimum = (prices['Low'] + prices['Close'].shift(1).bfill()).mean()\n","    high = prices['High'].rolling(14, min_periods=1).mean()\n","    low = prices['Low'].rolling(14, min_periods=1).mean()\n","\n","    # Tính chỉ số\n","    indi = pd.Series((maximum - minimum) / (high - low), name=\"VR\", index=prices.index)\n","    signal = pd.Series((indi > 0.5).astype(int), name=\"VR_signal\", index=prices.index)\n","    return indi, signal\n","\n","# Hàm tính giá trị Trix\n","def trix(prices):\n","    indi = TA.TRIX(prices, 10)\n","    signal = pd.Series((indi < 0).astype(int), name=\"TRIX_signal\", index=prices.index)\n","    return indi, signal"]},{"cell_type":"markdown","metadata":{"id":"7YersHs4xk-T"},"source":["# Dưới đây là cáC HÀM DÙNG CHO VIỆC STACK MODEL VÀ LOẠI BỎ CÁC OUTLIER DÙNG CÔNG THỨC IQR"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"iJ_v7LojjUKC","executionInfo":{"status":"ok","timestamp":1707051829832,"user_tz":-420,"elapsed":393,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}}},"outputs":[],"source":["\n","# hàm này để đánh giá mô hình tổng\n","# Là khi chúng ta train ra hết 3 model chúng ta xem rằng chúng kết hợp với nhau thì kết quả cuối cùng độ chính xác là bao nhiêu\n","def CV_ensemble(ensemble_name, ensemble_func, estimators, X_train, y_train, n_folds=5, shuffle=True, random_state=2022):\n","  kf = KFold(n_splits=5, random_state=random_state, shuffle=True) # Tạo số lượng dữ liệu bằng Kfold sau đó shuffle\n","\n","  res_list = [] # tạo res_list lưu thông tin của từng lần chia kfold\n","  # Chạy tập dữ liệu Kfold\n","  for train_idx, valid_idx in notebook.tqdm(kf.split(X_train), total=kf.get_n_splits(), desc='Eval_CV'): # Chạy theo số lần n_splits\n","    X_train_train, X_valid = X_train[train_idx], X_train[valid_idx]\n","    y_train_train, y_valid = y_train[train_idx], y_train[valid_idx]\n","    # Tổng hợp kết quả test đối vơi từng cặp k riêng\n","    ensemble_pred_proba = ensemble_func(estimators, X_train_train, y_train_train, X_valid)\n","\n","\n","    # Lấy các đánh giá  của model với tập train test k này\n","    neg_log_loss = np.negative(log_loss(y_valid, ensemble_pred_proba))\n","    accuracy = accuracy_score(y_valid, ensemble_pred_proba.argmax(axis=1))\n","\n","    res_list.append([ensemble_name, neg_log_loss, accuracy]) # Lưu kết quả lại`\n","  res_df = pd.DataFrame(np.vstack((res_list)))\n","  res_df.columns = ['model', 'log_loss', 'accuracy']  # thêm các thông số để đánh giá\n","  return res_df.reset_index(drop=True)\n","\n","\n","# hàm này để dự đoán bằng cách tổng hợp 3 model có train lại\n","def ensemble_average(estimators, X_train, y_train, X_test):\n","  # Hàm này vựa trên việc lấy xác suất của các model vote cái nào có xác suất cao nhất thì chọn\n","  preds = []\n","  num_estimators = len(estimators)\n","  num_class = len(np.unique(y_train))\n","  for iter in range(num_estimators):\n","    y_train = np.array(y_train, dtype=np.int64)\n","    estimators[iter].fit(X_train, y_train)\n","    preds.append(estimators[iter].predict_proba(X_test))\n","\n","\n","  preds_stack = np.hstack((preds))\n","  preds_mean = []\n","  for iter in range(num_class):\n","    col_idx = np.arange(iter, num_estimators * num_class, num_class)\n","    preds_mean.append(np.mean(preds_stack[:,col_idx], axis=1))\n","\n","  avg_pred = np.vstack((preds_mean)).transpose()\n","  return avg_pred\n","\n","\n","# hàm này đưa ra dự đoán mà ko cần train lại\n","def ensemble_average_model(estimators, X_test, y_train): # esitamtors là các model và X_test là dữ liệu train\n","  preds = []\n","  num_estimators = len(estimators)\n","  num_class = len(np.unique(y_train))\n","  # đoạn này lấy tổng dữ đoán của các model trong estimators\n","  for iter in range(num_estimators):\n","\n","    preds.append(estimators[iter].predict_proba(X_test))\n","\n","  preds_stack = np.hstack((preds))\n","  preds_mean = []\n","  # Dự đoán trung bình cho mỗi nhãn\n","  for iter in range(num_class):\n","    col_idx = np.arange(iter, num_estimators * num_class, num_class) # Tính cho số cột cho nhãn được dự đoán\n","    preds_mean.append(np.mean(preds_stack[:,col_idx], axis=1)) # Tính giá trị trung bình cho dự đoán\n","\n","  avg_pred = np.vstack((preds_mean)).transpose() # Lấy hết các giá trị tring bình sau đó chuyển vị\n","  return avg_pred\n","\n","\n","# hàm tạo ngưỡng nhiễu\n","def outlier_threshold(normality, k=1.5):\n","  q1 = np.quantile(normality, 0.2)\n","  q3 = np.quantile(normality, 0.8)\n","  threshold = q1 - k*(q3-q1) # Công thưc tạo ngưỡng nhiễu\n","  return threshold"]},{"cell_type":"markdown","metadata":{"id":"N4qIHyxY0Jfu"},"source":["# Khởi tạo và xử lý dữ liệu"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"pfAVHwSl0NBG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707051833469,"user_tz":-420,"elapsed":910,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}},"outputId":"66736a90-95f7-4e2b-88cd-460fa4ecf93b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/finta/finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n","/usr/local/lib/python3.10/dist-packages/finta/finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n","/usr/local/lib/python3.10/dist-packages/finta/finta.py:399: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  for x, y in zip(x.fillna(0).iteritems(), y.iteritems()):\n"]}],"source":["\n","def create_data(symbols, data_forex):\n","  price = {}\n","  # Lấy data của giá đóng cửa của loại đồng tiền\n","  for symbol in data_forex.keys():\n","      price[symbol] = data_forex[symbol]['Close'].copy()\n","\n","  y = {}\n","  for symbol in data_forex.keys():\n","\n","      y[symbol] = data_forex[symbol][\"Close\"].ffill()\n","\n","  # x là features\n","  X = {}\n","  # Đoạn này lấy dữ liệu X là data dùng để tách train, test\n","  for symbol in data_forex.keys():\n","      X_base_features = create_indicators(data_forex[symbol])\n","      X[symbol] = add_past_days_as_feature(data=X_base_features, n_days=LOOK_BACK)\n","      X[symbol][\"label\"] = y[symbol].copy()\n","      X[symbol] = X[symbol].dropna(axis=0)\n","      y[symbol] = X[symbol][\"label\"].copy()\n","\n","  return X, y\n","\n","X, y  = create_data(symbols, data_forex)\n","    # print(symbol, X[symbol].shape)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"fQndo_Oh0vHO","executionInfo":{"status":"ok","timestamp":1707051835073,"user_tz":-420,"elapsed":351,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","config = {\n","    'data_name': '3d_forex',\n","    'random_state': 2023\n","}\n","# Hàm này tách dữ liệu và sau đó chuẩn hóa riêng cho từng mã\n","def split_data_scale_2(X: dict, y: dict, symbols: dict):\n","    data_all_money = {}\n","    for symbol in symbols[\"all\"]:\n","        # Lấy dữ liệu gốc khi (chưa scale)\n","        data_original = X[symbol].copy()\n","        y_ = pd.Series(y[symbol])\n","        # Khởi tạo biến để lưu dữ liệu train và test cho từng fold của từng loại tề\n","        data_train_test = []\n","\n","        # Chia dữ liệu thành tập huấn luyện và tập kiểm tra theo tỷ lệ 90/10\n","        X_train, X_test, y_train, y_test = train_test_split(data_original, y_, test_size=0.1,shuffle =False, random_state=config['random_state'])\n","        scaler = MinMaxScaler()\n","        X_train_scaled = scaler.fit_transform(X_train)\n","        X_test_scaled = scaler.fit_transform(X_test)\n","\n","        data_all_money[symbol] = [X_train_scaled, X_test_scaled, y_train, y_test]\n","    return data_all_money # Dữ liệu được lưu ở dạng dict và 1 loại đồng tiền chưa 4 dữ liệu train và test của X và y\n","\n","# data_all_money = split_data_scale_2(X, y, symbols) # Lấy dữ liệu đã scale ra\n","\n","\n","\n","#  Ta đi smooth data để smooth ta cần biết các dữ liệu nhiễu thường xuyên, và bộ dữ liệu thực tế\n","# Việc cần ở đây là chỉ số nhiễu tượng trưng cho dữ liệu đó chỉ cần chỉnh bao nhiêu phần trăm bị nhiễu\n","def Smoothing_data(y, symbols) -> dict:\n","  symbolsmooths = {}\n","  # cho nhiều bộ smooths y khác nhau để xem cái nào tốt nhất\n","  for syms in symbols[\"all\"]:\n","    smooths = []\n","    for isf in range(4,5):\n","        for ins in range(4,5):\n","            smoothing_factor = isf\n","            n_seasons = ins\n","\n","\n","            # --- define state transition matrix A tạo ma trận n_season+1\n","            state_transition = np.zeros((n_seasons+1, n_seasons+1))\n","            # hidden level         Đặt mặc định số đầu tiên là 1\n","            state_transition[0,0] = 1\n","            # season cycle         Sau đó ta chỉnh các thông sô khác cho mặc định dòng 1 có n_season = -1\n","            state_transition[1,1:-1] = [-1.0] * (n_seasons-1)\n","\n","            # Tạo đường chéo chính có giá trị là 1\n","            state_transition[2:,1:-1] = np.eye(n_seasons-1)\n","\n","            # --- observation model H\n","            # observation is hidden level + weekly seasonal compoenent\n","            observation_model = [[1,1] + [0]*(n_seasons-1)]\n","\n","            # --- noise models, parametrized by the smoothing factor\n","            level_noise = 0.2 / smoothing_factor\n","            observation_noise = 0.2\n","            season_noise = 1e-3\n","\n","            process_noise_cov = np.diag([level_noise, season_noise] + [0]*(n_seasons-1))**2\n","            observation_noise_cov = observation_noise**2\n","            # Sử dụng Kalman filter\n","            kf = simdkalman.KalmanFilter(\n","                state_transition, # Hiển thị cách model chuyển tiếp để thay đổi ( Đây là cách nhìn dữ liệu bị nhiễu)\n","                process_noise_cov, # ma trận biểu thị mức độ nhiễu của mô hình (Đây là để biết nhiễu thường xảy ra với tuần suât nào)\n","                observation_model, #  Là mô hình quan sát của hệ thông thường là ma trận hay hàm số biểu thị (Đây là  kết quả hiện tại đang cần chỉnh nhiễu)\n","                observation_noise_cov #  Ma trận biểu thị mức độ nhiễu trong đo lường  (Đây giống như là sai số để xem kết quả dự đoán với thực tế thì sau bnhieu)\n","                )\n","            # Sau đó ta tính toán lấy ra được các dự đoán nhiễu và trã về dự đoán bị nhiễu được quy định bằng các tham số trên\n","            block = y[syms]\n","            n_train = block.shape[0]\n","            n_test = 60\n","            result = kf.compute(block, n_test)\n","            predictproba = result.smoothed.states.mean[:,0]\n","            y_label = []\n","            for ivalue in range(1,len(predictproba)):\n","                if(predictproba[ivalue] > predictproba[ivalue-1]):\n","                    y_label.append(1)\n","                else:\n","                    y_label.append(-1)\n","\n","            smooths.append(y_label)\n","    symbolsmooths[syms] = smooths\n","  return symbolsmooths\n","\n","# symbolsmooths_y = Smoothing_data(y,symbols)\n","\n","\n","def smooth_label(y_train_ ,symbolsmooths, syms ):\n","  # Hàm chỉnh dữ liệu lại cho smooth\n","  y_train = []\n","  y_test = []\n","  # chạy từng mã gán lại nhãn đã smooth, -1 là 0 giảm, 1 là 1 tăng\n","  for idx in range(len(y_train_)):\n","      if symbolsmooths[syms][0][idx]  == -1:\n","          y_train.append(0)\n","      else:\n","          y_train.append(1)\n","  # y_test cũng vậy, phần test sẽ là phần còn lại tính từ n_split\n","  for idx in range(len(symbolsmooths[syms][0]) - len(y_train_)):\n","      if symbolsmooths[syms][0][idx+ len(y_train_)] == -1:\n","          y_test.append(0)\n","      else:\n","          y_test.append(1)\n","  y_test.append(1)\n","  return y_train, y_test\n","\n","\n","def create_train_test_all_data_forex(X, y, symbols):\n","  data_all = {}\n","  data_all_money = split_data_scale_2(X,y, symbols)\n","  symbolsmooth_y = Smoothing_data(y, symbols)\n","\n","  for sym in symbols[\"all\"]:\n","    # Lấy các cột của dữ liệu\n","    df = pd.DataFrame(X[sym] )\n","    col_df = df.columns\n","\n","    # Lấy đúng loại tiền cần dùng\n","    X_train, X_test, y_train, y_test = data_all_money[sym]\n","    # Sau khi smooth data ta có y_train và y_test mới\n","    y_train, y_test = smooth_label( y_train, symbolsmooth_y, sym)\n","\n","    # Sau đó  tạo ra các dataframe mới\n","    X_train = pd.DataFrame(X_train, columns = col_df)\n","    X_test =  pd.DataFrame(X_test, columns = col_df)\n","    # Drop các cột không cần thiết là label đi\n","    X_train = X_train.drop(columns = [\"label\"])\n","    X_test = X_test.drop(columns = [\"label\"])\n","    # Lấy dữ liệu tiền tệ đó ra\n","    data_set = pd.get_dummies(X_train, drop_first=False)\n","    data_all[sym] = [data_set, X_test, y_train, y_test]\n","  return data_all\n","\n","data_all = create_train_test_all_data_forex(X, y, symbols)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ROVcHBG2JxmL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707051513518,"user_tz":-420,"elapsed":11,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}},"outputId":"115a8e54-d5b2-4c8b-f766-c2431ec28708"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2251"]},"metadata":{},"execution_count":9}],"source":["len(data_all[\"EURCHF\"][2])"]},{"cell_type":"markdown","metadata":{"id":"sw_g-vMT3myc"},"source":["# Tuning hyperparameter cho các model và loại bỏ outlier trong dữ liệu:\n","1. Random forest\n","2. XGBoost\n","3. MLP"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Vm_dBd3xP8Wl","executionInfo":{"status":"ok","timestamp":1707051837875,"user_tz":-420,"elapsed":382,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}}},"outputs":[],"source":["\"\"\"Code dưới đây loại bỏ các outlier khi vào việc train dữ liệu\"\"\"\n","\n","from skorch.net import chain\n","# Hàm khởi tạo Isolation forest cho việc loại bỏ các outlier vựa trên decision Tree\n","# Nó chỉ loại bỏ 1 phần bị outliers\n","def create_clf(X_train):\n","  clf = IsolationForest(\n","    n_estimators=100,\n","    max_samples='auto',\n","    n_jobs=-1,\n","    random_state=config['random_state'])\n","\n","  clf.fit(X_train)\n","  return clf\n","\n","\n","# Hàm loại bỏ outlier cho tập data train\n","def delete_outlier(clf, X_train, y_train):\n","  # Lấy ngưỡng cho dữ liệu\n","  normality_df = pd.DataFrame(clf.decision_function(X_train), columns=['normality'])\n","  threshold = outlier_threshold(normality_df['normality'].values, k=1.5)\n","\n","  # Sau đo ta loại bỏ dòng bị outlier\n","  X_train = X_train[normality_df['normality'].values>=threshold]\n","  y_train = y_train[normality_df['normality'].values>=threshold]\n","  return X_train, y_train\n","\n","# hàm này tạo ra dataframe chưa bộ thông số tốt nhất cho từng model\n","def create_best_cv(model_name):\n","\n","  model_list = []\n","  # mỗi model chuẩn bị 5 cái loss tốt nhất vì chạy CV\n","  for name in model_name:\n","    model_list.append(np.full(5, name))\n","\n","  best_cv_df = pd.DataFrame({'model': np.hstack((model_list)), 'accuracy':None, 'best_hyper_param':None})\n","  return best_cv_df\n","\n","def data_np(data_all, symbols):\n","  for sym in symbols[\"all\"]:\n","    # Thay đổi dữ liệu về dạng làm tròn và numpy\n","\n","    data_all[sym][2] = np.array(data_all[sym][2])\n","    data_all[sym][3] = np.array(data_all[sym][3])\n","\n","    data_all[sym][0] = data_all[sym][0].astype(np.float32)\n","    data_all[sym][1] = data_all[sym][1].astype(np.float32)\n","\n","\n","# Tạo model loại bỏ nhiễu\n","\n","# Tối ưu model\n","# Làm sao cho model chạy hiệu quả nhất và là no phải có lời\n","\n","def delete_and_create_best(data_all, symbols):\n","  data_np(data_all, symbols)\n","  best_cv_df = {}\n","\n","  for sym in symbols[\"all\"]:\n","    # Tạo clf cho delete outlier\n","    clf = create_clf(data_all[sym][0])\n","    # xóa các outliers\n","    print(sym)\n","    data_all[sym][0], data_all[sym][2] = delete_outlier(clf, data_all[sym][0], data_all[sym][2] )\n","    # Tạo ra best cv cho từng mã tiền\n","    model_name = [ 'rf', 'xgb', 'mlp']\n","    best_cv_df[sym] = create_best_cv(model_name)\n","\n","  return best_cv_df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vZe2UvjRSsl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706325569106,"user_tz":-420,"elapsed":359,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}},"outputId":"42d36ef2-120a-47d4-c00f-e92ee6a8d84e"},"outputs":[{"output_type":"stream","name":"stdout","text":["EURCHF\n"]}],"source":["best_cv_df = delete_and_create_best(data_all, symbols)"]},{"cell_type":"markdown","metadata":{"id":"8kbgi1WW8KUZ"},"source":["# Đây chưa các hàm check cho model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MusWcx3xZEDC"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def calculate_accuracy(y_true, y_pred):\n","    \"\"\"\n","    Tính độ chính xác giữa dự đoán và nhãn thực tế.\n","\n","    Parameters:\n","    - y_true: Mảng chứa nhãn thực tế.\n","    - y_pred: Mảng chứa dự đoán của mô hình.\n","\n","    Returns:\n","    - accuracy: Độ chính xác.\n","    \"\"\"\n","    accuracy = accuracy_score(y_true, y_pred)\n","    return accuracy\n","\n","\n"]},{"cell_type":"markdown","source":["# Bên dưới khai bó tất cả các thư viện có model nhất\n"],"metadata":{"id":"xlv4fVohB0kC"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier, StackingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.linear_model import OrthogonalMatchingPursuit, PassiveAggressiveClassifier\n"],"metadata":{"id":"vCUmVOplAtk4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# QUy trình đầu tiên ta sẽ lấy độ chính xác của model cơ sở tốt nhất\n","# Sau đó ta lấy train test cho model mới\n","# Tiếp theo ta làm công việc chạy predict cho model mới khi stack lại với nhau\n","# Cuối cùng ta so sánh accuracy của nó với model mới và trả về True False theo độ chính xác"],"metadata":{"id":"slW-rMJQEHsW"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"kh4N5hjIBsj1","executionInfo":{"status":"ok","timestamp":1707051619996,"user_tz":-420,"elapsed":7,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier, StackingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.linear_model import OrthogonalMatchingPursuit, PassiveAggressiveClassifier\n","\n","# def all_model_sklearn(X_train, y_train):\n","#     # Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n","#     X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","#     # Danh sách các mô hình phân loại\n","#     classifiers = {\n","#         'KNeighborsClassifier': KNeighborsClassifier(),\n","#         'LogisticRegression': LogisticRegression(),\n","#         'SGDClassifier': SGDClassifier(),\n","#         'SVC': SVC(),\n","#         'DecisionTreeClassifier': DecisionTreeClassifier(),\n","#         'ExtraTreeClassifier': ExtraTreeClassifier(),\n","#         'GaussianNB': GaussianNB(),\n","#         'MultinomialNB': MultinomialNB(),\n","#         'ComplementNB': ComplementNB(),\n","#         'BernoulliNB': BernoulliNB(),\n","\n","#         'AdaBoostClassifier': AdaBoostClassifier(),\n","#         'GradientBoostingClassifier': GradientBoostingClassifier(),\n","#         'BaggingClassifier': BaggingClassifier(),\n","#         'VotingClassifier': VotingClassifier(estimators=[('svc', SVC()), ('dt', DecisionTreeClassifier()), ('rf', RandomForestClassifier())]),\n","#         'StackingClassifier': StackingClassifier(estimators=[('svc', SVC()), ('dt', DecisionTreeClassifier()), ('rf', RandomForestClassifier())]),\n","#         'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n","\n","#         'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n","#         'MLPClassifier': MLPClassifier(),\n","#         'GaussianProcessClassifier': GaussianProcessClassifier(),\n","#         'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(),\n","#         'PassiveAggressiveClassifier': PassiveAggressiveClassifier(),\n","#         'Perceptron': Perceptron(),\n","#     }\n","\n","#     # Huấn luyện và đánh giá từng mô hình\n","#     for name, model in classifiers.items():\n","#         model.fit(X_train, y_train)\n","\n","#     return classifiers\n","\n","# # Gọi hàm với dữ liệu X_train và y_train của bạn\n","# for sym in symbols[\"all\"]:\n","#     X_train = data_all[sym][0]\n","#     y_train = data_all[sym][2]\n","\n","#     all_classifier_model = all_model_sklearn(X_train, y_train)\n"]},{"cell_type":"code","source":["\n","X_train, X_test, y_train, y_test = data_all[\"EURCHF\"]\n","base_classifier =  DecisionTreeClassifier(max_depth =1)\n","adaboost = AdaBoostClassifier(base_classifier, n_estimators = 50, random_state = 2023)\n","# Train model adaboost\n","\n","adaboost.fit(X_train, y_train)\n","y_pred = adaboost.predict(X_test)\n","\n","acc =  accuracy_score(y_test, y_pred)\n","print(\"Độ chính xác của model adaboost là: \", acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uwK6B_jNLoL6","executionInfo":{"status":"ok","timestamp":1707051853194,"user_tz":-420,"elapsed":7591,"user":{"displayName":"0330- Lê Nguyễn Anh Nhật","userId":"07393004795663485962"}},"outputId":"ebf2aeaf-9598-4ee2-b6c6-7f27de4b60d5"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Độ chính xác của model adaboost là:  0.6374501992031872\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VT6VPleMGmh4"},"outputs":[],"source":["# def tune_and_evaluate(X_train, y_train, model, param_grid):\n","#     X_train, y_train\n","\n","#     grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n","#     grid_search.fit(X_train, y_train)\n","\n","#     best_model = grid_search.best_estimator_\n","#     best_params = grid_search.best_params_\n","\n","\n","#     return best_model, best_params, accuracy\n","\n","# # Danh sách các mô hình cùng với các tham số cần tinh chỉnh\n","# models = {\n","#     'SVC': (SVC(), {'C': [1, 10, 100], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}),\n","#     'LogisticRegression': (LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1, 10, 100]}),\n","#     'KNeighborsClassifier': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}),\n","#     'DecisionTreeClassifier': (DecisionTreeClassifier(), {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30]}),\n","#     'GaussianNB': (GaussianNB(), {}),\n","#     'AdaBoostClassifier': (AdaBoostClassifier(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}),\n","#     'GradientBoostingClassifier': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}),\n","# }\n","\n","# # Gọi hàm tuning và đánh giá cho từng mô hình\n","# for model_name, (model, param_grid) in models.items():\n","#     best_model, best_params, accuracy = tune_and_evaluate(X_train, y_train, model, param_grid)\n","#     print(f\"Results for {model_name}:\\nBest Params: {best_params}\\nAccuracy: {accuracy}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgEz1AWvLx8j"},"outputs":[],"source":["# buy, sell và hold\n","# Dùng ảnh 1 giờ nhìn về 4 giờ tương lai tăng 1 ngưỡng nhất định thì cho là buy thường 700 điểm -700 cho là sell.\n","# Cái cách dùng 0."]},{"cell_type":"markdown","source":["# Muốn làm thế này phả cs model giữ lại nhưng gì cần thiết như data và model"],"metadata":{"id":"iYda48kbAFge"}}],"metadata":{"colab":{"provenance":[{"file_id":"1Px06-jqUNyjVoQEdxvmfLbj9cMZiJxCg","timestamp":1707051155314},{"file_id":"186aBy2xNDEdSpFuB_eq0F_n1P05W3Pzg","timestamp":1706276274801}],"history_visible":true,"authorship_tag":"ABX9TyOqc5kCzPNcH772Lo7+jr/2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}